{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization in Neural Networks\n",
    "\n",
    "**Weight Initialization** refers to the process of setting the initial values of the weights in a neural network before training begins. Proper initialization is critical for training deep learning models effectively, as it affects the speed of convergence and the network's ability to learn.\n",
    "\n",
    "---\n",
    "\n",
    "## Importance of Weight Initialization\n",
    "1. **Avoiding Vanishing/Exploding Gradients**:\n",
    "   - Poor initialization can lead to gradients becoming too small (vanishing) or too large (exploding) during backpropagation, hindering effective learning.\n",
    "2. **Faster Convergence**:\n",
    "   - Proper initialization helps the optimizer find a good starting point, speeding up training.\n",
    "3. **Preventing Symmetry**:\n",
    "   - If all weights are initialized to the same value, the neurons will learn identical features, reducing the model's capacity. Initialization ensures that weights start with slight variations.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Weight Initialization Methods\n",
    "\n",
    "### 1. **Random Initialization**\n",
    "- Weights are initialized randomly, usually from a uniform or normal distribution.\n",
    "- Example:\n",
    "  - \\( w \\sim \\mathcal{U}(-a, a) \\) (Uniform distribution)\n",
    "  - \\( w \\sim \\mathcal{N}(0, \\sigma^2) \\) (Normal distribution)\n",
    "\n",
    "### 2. **Zero Initialization (Not Recommended)**\n",
    "- All weights are initialized to zero.\n",
    "- Problem: Leads to symmetry, where all neurons in a layer learn the same features, making the network ineffective.\n",
    "\n",
    "### 3. **Xavier Initialization (Glorot Initialization)**\n",
    "- Designed for sigmoid or tanh activation functions.\n",
    "- Ensures that the variance of the inputs and outputs remains the same across layers.\n",
    "- Formula:\n",
    "  - For uniform distribution: \\( w \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}) \\)\n",
    "  - For normal distribution: \\( w \\sim \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}}) \\)\n",
    "  - \\( n_{in} \\): Number of input units to the layer.\n",
    "  - \\( n_{out} \\): Number of output units from the layer.\n",
    "\n",
    "### 4. **He Initialization (He et al.)**\n",
    "- Designed for ReLU and its variants (e.g., Leaky ReLU).\n",
    "- Ensures better flow of gradients for layers using ReLU activations.\n",
    "- Formula:\n",
    "  - For normal distribution: \\( w \\sim \\mathcal{N}(0, \\frac{2}{n_{in}}) \\)\n",
    "  - For uniform distribution: \\( w \\sim \\mathcal{U}(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}}) \\)\n",
    "\n",
    "### 5. **LeCun Initialization**\n",
    "- Designed for activation functions like SELU (Scaled Exponential Linear Unit).\n",
    "- Formula:\n",
    "  - \\( w \\sim \\mathcal{N}(0, \\frac{1}{n_{in}}) \\)\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Use in Frameworks\n",
    "\n",
    "### In TensorFlow/Keras\n",
    "```python\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import RandomNormal, GlorotUniform, HeNormal\n",
    "\n",
    "# Example of weight initialization\n",
    "layer = Dense(\n",
    "    units=128,\n",
    "    activation='relu',\n",
    "    kernel_initializer=HeNormal()  # Using He initialization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Initialization Techniques\n",
    "\n",
    "| Initialization Method | Best For             | Formula                                               |\n",
    "|------------------------|----------------------|-------------------------------------------------------|\n",
    "| **Random**            | General use         | Uniform/Normal distribution                           |\n",
    "| **Xavier**            | Sigmoid, Tanh       | \\( \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}}) \\)     |\n",
    "| **He**                | ReLU, Leaky ReLU    | \\( \\mathcal{N}(0, \\frac{2}{n_{in}}) \\)               |\n",
    "| **LeCun**             | SELU               | \\( \\mathcal{N}(0, \\frac{1}{n_{in}}) \\)               |\n",
    "\n",
    "---\n",
    "\n",
    "## Explanation of Notations\n",
    "- \\( \\mathcal{N}(0, \\sigma^2) \\): Normal distribution with a mean of 0 and variance \\( \\sigma^2 \\).\n",
    "- \\( n_{in} \\): Number of input units to the layer.\n",
    "- \\( n_{out} \\): Number of output units from the layer.\n",
    "\n",
    "These initialization methods are designed to maintain stable gradients during training and are chosen based on the type of activation function used in the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"The End)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
